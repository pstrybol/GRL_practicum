> Question 1: As in classical machine learning we thus need to construct a set of edges to train on and a set of edges to test our performance on. However we can't just randomly divide all the positive edges in train and test, think about why this is not a valid option? Figure 2 gives a hint towards the solution ...

Answer 1: To ensure the model sees every node in the graph in at least 1 positive interaction we need to construct the minimum spanning tree (MST). The MST connects all nodes in the graph, without cycles with the minumum number of edges, this ensures all nodes are seen at least once during training and thus have a learned node representation. 

> Question 2: Another important aspect to take into account is the construction of negative samples. What would represent a negative sample in the case of a PPI network and how can we sample this? Also, how many negatives would it take to represent the actual situation, is it wise to sample this many or do we need a separate hyperparameter for this?

Answer 2: In case of the PPI network a negative sample, or a non-edge, represents two proteins who are certainly not interacting. Naivly, we could sample negative edges from all node pairs that are not connected in the original graph. However, this is (in most applications) not the most optimal way of sampling negatives. In this case, there is almost no way of knowing for sure which proteins are not interacting hence it is assumed the PPI graph is a dynamic graph, i.e. not all TP and TN are known beforehand. If there is prior knowledge that allows you to sample *better* negatives this should be used, for example with PPIs there exist a negatome database that captures known non-interacting proteins through e.g. literature search. Note that the negative sampling can significantly alter the model's performance.

> Question 3: Note that there is one layer not visualized in Figure 3 but essential specifically for link prediction. Can you guess what is missing?

Answer 3: Since the objective is to predict interactions between nodes, we also need to construct edge embeddings from the input node pair. This represenetations than represents the distance between the input nodes. If this distance is small, i.e. the nodes are situated close to each other in the latent space, the output probability of these two nodes interacting should be high, and vice versa. If not, the backpropagation step of the model will alter the weights accordingly.

> Question 4 & 5: Could explain why the validation loss is lower at the start of training? Why do we include Average Precision as a performance metric and why is it lower than the AUC?

Answer 4: This is an *artifact* of how Keras calculates the training and validation loss. For training, the loss is the average over all batch-losses while the validation loss is calculated at once, at the end of every epohc. This means that when we calculate the first validation loss the model already performed several updates and thus it's expected for the validation loss to be lower than the traning loss in the beginning. This question is mainly to prove that you should not mindlessly copy-paste code or functions from packages, understand what is going on under the hood so you can explain the behaviour of your model

Answer 5: Most graphs are quite sparse and the number of possible negatives often vastly outweighs the number of positive that are present. You can check this by calcualting the number of possible combinations between the number of nodes and substracting the number of positives, this would lead you to the intrinic negative to postive ratio. To mimick this behaviour we input the negative:postive ratio as a tunable hyperparamter. Consequently, we have a (large) class imbalance and the AUC can not deal with this appropriately. Looking on the x-axis the False Positive Rate (FPR) is plotted, this takes into account the number of TN (FP/FP+TN). And for graphs, the number of TN >>> TP so even if the model predict everything as negative, it will still achieve acceptable performance but in actuality, it is a bad model. Average precision does not take this into account as precision is calculated as TP/TP+FP, i.e. positive predictive value. This metric is far more suited to assess the ability of a model to correctly predict positve samples. 

> Question 6: Often in ML or DL when gradient descent (GD) algoritms are used to optimze the objective function, GD will converge faster if all features have the same scale, i.e. are normalized. Check how the feature matrix is normalized and reason why this is done a particular way and identify the function in pytorch geometric that could perform this operation.

Answer 6: As stated in the question, GD algorithms converge faster when all features have the same scale. But that is not the only reason why. Since the original feature matrix is made up of word counts, we are dealing with count data. And to the proportion of each feature in a certain sample we should row-normalize. A more detailed and very well explained answer can be found here: https://stackoverflow.com/a/60275448. Function in pytorch geometric: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.NormalizeFeatures.html#torch_geometric.transforms.NormalizeFeatures .


> Optional question: Also, keep in mind that later on we will want to visualize our embeddings so be sure to either return your embeddings or write a function such that you can obtain your embeddings after training. What is the difference between these options?

This is again an example of understanding the functions you are using from other software pacakges such as PyTorch Geometric. The GCNconv layer returns the node embeddings but not the actual class probabilities. If you want to use the predicted probabilities downstream we need an additional actication function and in the case of multiclass classification this would be a softmax.

> Question 7: Note one peculiarity in the testing loop: model.eval(), why is this neceassary and what is it's effect on the model?

Answer 7: This tells the model to run in evaluation mode which means all regularization will be turned off, in this case it would only affect the dropout layers. These are only used in training to prevent the model from overfitting and help robustness. However, in validation (and testing) we want to assess the model's performance using the entire cability and thus we run `model.eval()` before the validation/testing loop.